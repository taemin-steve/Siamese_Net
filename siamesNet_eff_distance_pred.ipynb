{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob\n",
    "import cv2 as cv\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import timm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_model_summary\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DataFrame\n",
    "\n",
    "## columns: review_img_path, product_img_path, label\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./masked_data/product_img\\\\0.jpg', './masked_data/product_img\\\\1.jpg', './masked_data/product_img\\\\10.jpg', './masked_data/product_img\\\\100.jpg', './masked_data/product_img\\\\12.jpg', './masked_data/product_img\\\\13.jpg', './masked_data/product_img\\\\14.jpg', './masked_data/product_img\\\\15.jpg', './masked_data/product_img\\\\20.jpg', './masked_data/product_img\\\\25.jpg', './masked_data/product_img\\\\28.jpg', './masked_data/product_img\\\\30.jpg', './masked_data/product_img\\\\33.jpg', './masked_data/product_img\\\\34.jpg', './masked_data/product_img\\\\35.jpg', './masked_data/product_img\\\\36.jpg', './masked_data/product_img\\\\37.jpg', './masked_data/product_img\\\\39.jpg', './masked_data/product_img\\\\4.jpg', './masked_data/product_img\\\\40.jpg', './masked_data/product_img\\\\41.jpg', './masked_data/product_img\\\\42.jpg', './masked_data/product_img\\\\43.jpg', './masked_data/product_img\\\\45.jpg', './masked_data/product_img\\\\46.jpg', './masked_data/product_img\\\\48.jpg', './masked_data/product_img\\\\49.jpg', './masked_data/product_img\\\\5.jpg', './masked_data/product_img\\\\50.jpg', './masked_data/product_img\\\\52.jpg', './masked_data/product_img\\\\53.jpg', './masked_data/product_img\\\\54.jpg', './masked_data/product_img\\\\59.jpg', './masked_data/product_img\\\\60.jpg', './masked_data/product_img\\\\61.jpg', './masked_data/product_img\\\\62.jpg', './masked_data/product_img\\\\63.jpg', './masked_data/product_img\\\\66.jpg', './masked_data/product_img\\\\68.jpg', './masked_data/product_img\\\\7.jpg', './masked_data/product_img\\\\73.jpg', './masked_data/product_img\\\\74.jpg', './masked_data/product_img\\\\77.jpg', './masked_data/product_img\\\\78.jpg', './masked_data/product_img\\\\79.jpg', './masked_data/product_img\\\\80.jpg', './masked_data/product_img\\\\82.jpg', './masked_data/product_img\\\\83.jpg', './masked_data/product_img\\\\84.jpg', './masked_data/product_img\\\\87.jpg', './masked_data/product_img\\\\88.jpg', './masked_data/product_img\\\\90.jpg', './masked_data/product_img\\\\92.jpg', './masked_data/product_img\\\\94.jpg', './masked_data/product_img\\\\96.jpg', './masked_data/product_img\\\\98.jpg']\n"
     ]
    }
   ],
   "source": [
    "all_product_img_list = glob.glob('./masked_data/product_img/*')\n",
    "all_product_img_list = sorted(all_product_img_list)\n",
    "print(all_product_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_img_path = []\n",
    "product_img_path = []\n",
    "label = []\n",
    "for product_img in all_product_img_list:\n",
    "    glob_img_list = []\n",
    "    str = product_img.split(\"\\\\\")[1] # for window\n",
    "    # str = product_img.split(\"/\")[3] # for linux\n",
    "    \n",
    "    str = str.split(\".\")[0]\n",
    "    glob_img_list = glob.glob('./masked_data/review_img/'+str +'_review_img/O/*.jpg')\n",
    "    review_img_path = review_img_path + glob_img_list\n",
    "    product_img_path = product_img_path + [product_img  for i in range(len(glob_img_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18749\n",
      "18749\n"
     ]
    }
   ],
   "source": [
    "print(len(product_img_path))\n",
    "print(len(review_img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(review_img_path)):\n",
    "    random_number = random.randint(0, 1)\n",
    "    if random_number == 0:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        review_num = review_img_path[i].split('\\\\')[1] # for Window \n",
    "        # review_num = review_img_path[i].split('/')[3] # for Linux\n",
    "        \n",
    "        review_num = review_num.split(\"_\")[0]\n",
    "        random_number = random.randint(0, len(all_product_img_list) - 1)\n",
    "        while review_num == random_number:\n",
    "            random_number = random.randint(0, len(all_product_img_list) - 1)\n",
    "        product_img_path[i] = all_product_img_list[random_number]\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_img_path</th>\n",
       "      <th>product_img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./masked_data/review_img/0_review_img/O\\1.jpg</td>\n",
       "      <td>./masked_data/product_img\\0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./masked_data/review_img/0_review_img/O\\107.jpg</td>\n",
       "      <td>./masked_data/product_img\\0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./masked_data/review_img/0_review_img/O\\109.jpg</td>\n",
       "      <td>./masked_data/product_img\\0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./masked_data/review_img/0_review_img/O\\111.jpg</td>\n",
       "      <td>./masked_data/product_img\\0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./masked_data/review_img/0_review_img/O\\119.jpg</td>\n",
       "      <td>./masked_data/product_img\\0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18744</th>\n",
       "      <td>./masked_data/review_img/98_review_img/O\\93.jpg</td>\n",
       "      <td>./masked_data/product_img\\82.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18745</th>\n",
       "      <td>./masked_data/review_img/98_review_img/O\\95.jpg</td>\n",
       "      <td>./masked_data/product_img\\98.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18746</th>\n",
       "      <td>./masked_data/review_img/98_review_img/O\\96.jpg</td>\n",
       "      <td>./masked_data/product_img\\98.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18747</th>\n",
       "      <td>./masked_data/review_img/98_review_img/O\\97.jpg</td>\n",
       "      <td>./masked_data/product_img\\83.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18748</th>\n",
       "      <td>./masked_data/review_img/98_review_img/O\\98.jpg</td>\n",
       "      <td>./masked_data/product_img\\43.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18749 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       review_img_path   \n",
       "0        ./masked_data/review_img/0_review_img/O\\1.jpg  \\\n",
       "1      ./masked_data/review_img/0_review_img/O\\107.jpg   \n",
       "2      ./masked_data/review_img/0_review_img/O\\109.jpg   \n",
       "3      ./masked_data/review_img/0_review_img/O\\111.jpg   \n",
       "4      ./masked_data/review_img/0_review_img/O\\119.jpg   \n",
       "...                                                ...   \n",
       "18744  ./masked_data/review_img/98_review_img/O\\93.jpg   \n",
       "18745  ./masked_data/review_img/98_review_img/O\\95.jpg   \n",
       "18746  ./masked_data/review_img/98_review_img/O\\96.jpg   \n",
       "18747  ./masked_data/review_img/98_review_img/O\\97.jpg   \n",
       "18748  ./masked_data/review_img/98_review_img/O\\98.jpg   \n",
       "\n",
       "                       product_img_path  label  \n",
       "0       ./masked_data/product_img\\0.jpg      0  \n",
       "1       ./masked_data/product_img\\0.jpg      0  \n",
       "2       ./masked_data/product_img\\0.jpg      0  \n",
       "3       ./masked_data/product_img\\0.jpg      0  \n",
       "4       ./masked_data/product_img\\0.jpg      0  \n",
       "...                                 ...    ...  \n",
       "18744  ./masked_data/product_img\\82.jpg      1  \n",
       "18745  ./masked_data/product_img\\98.jpg      0  \n",
       "18746  ./masked_data/product_img\\98.jpg      0  \n",
       "18747  ./masked_data/product_img\\83.jpg      1  \n",
       "18748  ./masked_data/product_img\\43.jpg      1  \n",
       "\n",
       "[18749 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['review_img_path','product_img_path', 'label'])\n",
    "df['review_img_path'] = review_img_path\n",
    "df['product_img_path'] = product_img_path\n",
    "df['label'] = label\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siames Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':1,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    # 'LEARNING_RATE':10,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = 'a'\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed ê³ ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, _, _ = train_test_split(df, df['label'], test_size=0.3, stratify=df['label'], random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self,review_img_path,product_img_path,label,transform=None):\n",
    "        self.review_img_path = review_img_path\n",
    "        self.product_img_path = product_img_path\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        review_img = cv.imread(self.review_img_path[index])\n",
    "        product_img = cv.imread(self.product_img_path[index])\n",
    "        review_img = cv.resize(review_img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "        product_img = cv.resize(product_img, (CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            review_img  = self.transform(image=review_img)['image']\n",
    "            product_img  = self.transform(image=product_img)['image']\n",
    "    \n",
    "        return review_img, product_img, self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.review_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = SiameseNetworkDataset(train[\"review_img_path\"].values, train[\"product_img_path\"].values, train[\"label\"].values, train_transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "val_dataset = SiameseNetworkDataset(df[\"review_img_path\"].values, df[\"product_img_path\"].values, df[\"label\"].values, train_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn']\n"
     ]
    }
   ],
   "source": [
    "avail_pretrained_models = timm.list_models('*VGG*',pretrained=True)\n",
    "print(avail_pretrained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model('efficientnet_b0', pretrained=False)\n",
    "        self.classifier = nn.Linear(1000, 50)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.ReLU = nn.ReLU(inplace=False)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = self.backbone(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        # x = self.ReLU(x)\n",
    "        \n",
    "        y = self.backbone(y)\n",
    "        # y = self.dropout(y)\n",
    "        y = self.classifier(y)\n",
    "        # y= self.ReLU(y)\n",
    "        \n",
    "        z = F.pairwise_distance(x, y, keepdim = True)\n",
    "        \n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    result = []\n",
    "    pp = []\n",
    "    pn = []\n",
    "    nn = []\n",
    "    n_p = []\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for review_img, product_img, labels in tqdm(iter(val_loader)):\n",
    "            review_img = review_img.float().to(device)\n",
    "            product_img = product_img.float().to(device)\n",
    "\n",
    "            labels = labels.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            pred = model(review_img, product_img)\n",
    "            # print(pred.shape)\n",
    "            \n",
    "            loss = criterion(pred, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            pred = pred.detach().cpu().numpy().tolist()\n",
    "            labels = labels.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            pred_list += pred\n",
    "            label_list += labels\n",
    "            \n",
    "            for i in range(len(pred)):\n",
    "                if labels[i] == 0:\n",
    "                    if pred[i][0] < 0.5:\n",
    "                        result.append(1) #ë§žê²Œ ì˜ˆì¸¡í•˜ë©´ 1\n",
    "                        pp.append(1)\n",
    "                    else:\n",
    "                        result.append(0)\n",
    "                        pn.append(1)\n",
    "                else:\n",
    "                    if pred[i][0] > 0.5:\n",
    "                        result.append(1) #ë§žê²Œ ì˜ˆì¸¡í•˜ë©´ 1\n",
    "                        nn.append(1)\n",
    "                    else:\n",
    "                        result.append(0)\n",
    "                        n_p.append(1)\n",
    "            # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "            \n",
    "        result_len = len(result)\n",
    "        print(sum(pp)/result_len)\n",
    "        print(sum(pn)/result_len)\n",
    "        print(sum(nn)/result_len)\n",
    "        print(sum(n_p)/result_len)\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_score = (sum(result)/result_len) * 100 \n",
    "        \n",
    "        \n",
    "    \n",
    "    return _val_loss, _val_score , pred_list , label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Contrastive Loss Function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, euclidean_distance, label):\n",
    "      # Calculate the euclidian distance and calculate the contrastive loss\n",
    "        # loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "        #                             (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        # loss_contrastive = ((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "        #                             (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        \n",
    "        loss_contrastive = []\n",
    "        \n",
    "        for i in range(len(label)):\n",
    "              loss_contrastive.append((1-label[i]) * pow(euclidean_distance[i][0], 2) +\n",
    "                                    (label[i]) * pow(max(self.margin - euclidean_distance[i][0],0), 2))\n",
    "        return sum(loss_contrastive)/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model = model.to(device)\n",
    "    # criterion = nn.NLLLoss(reduction=\"sum\").to(device)\n",
    "    criterion = ContrastiveLoss().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(0, CFG['EPOCHS']):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        flag_list = []\n",
    "        \n",
    "        # for review_img, product_img, labels in tqdm(iter(train_loader)):\n",
    "            \n",
    "        #     review_img = review_img.float().to(device)\n",
    "        #     product_img = product_img.float().to(device)\n",
    "                  \n",
    "        #     labels = labels.type(torch.LongTensor).to(device)\n",
    "            \n",
    "        #     optimizer.zero_grad()\n",
    "        #     output = model(review_img, product_img)\n",
    "        #     # print(output)\n",
    "        #     # print(output[3][0])\n",
    "            \n",
    "        #     # print(labels)\n",
    "        #     # print(labels[3])\n",
    "            \n",
    "        #     loss = criterion(output, labels)\n",
    "        #     # print(loss)\n",
    "            \n",
    "        #     # output = output.detach().cpu().numpy().tolist()\n",
    "        #     # labels = labels.detach().cpu().numpy().tolist()\n",
    "        #     # print(output)\n",
    "        #     # print(output[3][0])\n",
    "        #     # print(labels)\n",
    "        #     # print(labels[1])\n",
    "        #     # print(loss)\n",
    "            \n",
    "        #     # print(loss.item())\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "        #     train_loss.append(loss.item())\n",
    "                    \n",
    "                \n",
    "        _val_loss, _val_score, prediction, labels = validation(model, criterion, val_loader, device)\n",
    "        \n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val accuracy score : [{_val_score:.5f}]')\n",
    "       \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_score < _val_score:\n",
    "            best_score = _val_score\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), \"./distance_EffNetBase_E_Contra.pt\")\n",
    "            \n",
    "            \n",
    "    return prediction , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (backbone): EfficientNet(\n",
       "    (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNormAct2d(\n",
       "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNormAct2d(\n",
       "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNormAct2d(\n",
       "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNormAct2d(\n",
       "      1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1000, out_features=50, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (ReLU): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.load_state_dict(torch.load('./distance_VGGBase_E_Contra.pt'))\n",
    "model.eval()\n",
    "# print(pytorch_model_summary.summary(model, torch.zeros(8,3,256,256),max_depth=None, show_parent_layers=True, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5acc0659a8944cdad517964cf354455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3370846445143741\n",
      "0.16651554749586644\n",
      "0.37415328817536936\n",
      "0.1222465198143901\n",
      "Epoch [0], Train Loss : [nan] Val Loss : [0.20442] Val accuracy score : [71.12379]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EHmin\\anaconda3\\envs\\EHmin\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\EHmin\\anaconda3\\envs\\EHmin\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "prediction, prediction_list = train(model, optimizer, None, val_loader, scheduler, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.41915902495384216],\n",
       " [0.21571287512779236],\n",
       " [0.5272825956344604],\n",
       " [0.20516470074653625],\n",
       " [0.5243050456047058],\n",
       " [0.8596328496932983],\n",
       " [0.2316335290670395],\n",
       " [0.7739569544792175],\n",
       " [0.29862770438194275],\n",
       " [0.6296398639678955],\n",
       " [1.9979442358016968],\n",
       " [0.024263517931103706],\n",
       " [1.177283763885498],\n",
       " [0.6349438428878784],\n",
       " [0.13758227229118347],\n",
       " [1.1583110094070435],\n",
       " [0.10598281025886536],\n",
       " [0.22385099530220032],\n",
       " [0.3061404526233673],\n",
       " [1.7865067720413208],\n",
       " [0.05046224221587181],\n",
       " [0.2809430956840515],\n",
       " [0.3586622178554535],\n",
       " [0.5584232807159424],\n",
       " [1.2825239896774292],\n",
       " [0.8185639977455139],\n",
       " [0.5535506010055542],\n",
       " [0.1259673684835434],\n",
       " [1.7557965517044067],\n",
       " [0.7386475205421448],\n",
       " [0.16405171155929565],\n",
       " [0.23525086045265198],\n",
       " [0.22323891520500183],\n",
       " [1.0298813581466675],\n",
       " [0.7197586297988892],\n",
       " [1.4889843463897705],\n",
       " [1.0818814039230347],\n",
       " [0.41915902495384216],\n",
       " [0.8164364695549011],\n",
       " [0.3655383288860321],\n",
       " [0.1659923940896988],\n",
       " [0.16176949441432953],\n",
       " [0.2574446499347687],\n",
       " [1.0070245265960693],\n",
       " [0.307910293340683],\n",
       " [0.22557108104228973],\n",
       " [1.1024852991104126],\n",
       " [0.4007549285888672],\n",
       " [1.3588755130767822],\n",
       " [1.4629102945327759],\n",
       " [0.2348259687423706],\n",
       " [1.2244641780853271],\n",
       " [0.2501540780067444],\n",
       " [0.1038462370634079],\n",
       " [1.0975332260131836],\n",
       " [1.5601297616958618],\n",
       " [0.9853063225746155],\n",
       " [3.0019278526306152],\n",
       " [0.070885069668293],\n",
       " [0.44270792603492737],\n",
       " [0.20667240023612976],\n",
       " [0.28601646423339844],\n",
       " [0.21434390544891357],\n",
       " [0.2908996343612671],\n",
       " [1.0113375186920166],\n",
       " [1.0530672073364258],\n",
       " [1.405548334121704],\n",
       " [0.599931538105011],\n",
       " [2.029261827468872],\n",
       " [0.45888176560401917],\n",
       " [0.3747536242008209],\n",
       " [0.9811219573020935],\n",
       " [0.9285459518432617],\n",
       " [1.288112759590149],\n",
       " [0.9719222784042358],\n",
       " [0.933085560798645],\n",
       " [0.0725817084312439],\n",
       " [0.26637235283851624],\n",
       " [0.19734477996826172],\n",
       " [0.6325074434280396],\n",
       " [0.23738469183444977],\n",
       " [0.15777279436588287],\n",
       " [0.17006410658359528],\n",
       " [0.4089857339859009],\n",
       " [0.08323373645544052],\n",
       " [0.4767225682735443],\n",
       " [0.4320782721042633],\n",
       " [1.2088987827301025],\n",
       " [1.5103085041046143],\n",
       " [0.09664567559957504],\n",
       " [0.9026573896408081],\n",
       " [0.16552476584911346],\n",
       " [0.06976211816072464],\n",
       " [0.20728325843811035],\n",
       " [0.24263042211532593],\n",
       " [0.4016968905925751],\n",
       " [1.4530420303344727],\n",
       " [0.21320801973342896],\n",
       " [0.9658448100090027],\n",
       " [0.9117180109024048],\n",
       " [0.3700226843357086],\n",
       " [0.19538572430610657],\n",
       " [0.19169946014881134],\n",
       " [0.8515132665634155],\n",
       " [0.6316809058189392],\n",
       " [0.8485944867134094],\n",
       " [0.39298152923583984],\n",
       " [1.3644115924835205],\n",
       " [0.21658803522586823],\n",
       " [0.11834314465522766],\n",
       " [0.20202985405921936],\n",
       " [0.9901231527328491],\n",
       " [0.8468324542045593],\n",
       " [0.3489156663417816],\n",
       " [0.6147627234458923],\n",
       " [0.23917421698570251],\n",
       " [1.3333078622817993],\n",
       " [1.170172095298767],\n",
       " [0.11481978744268417],\n",
       " [0.17004448175430298],\n",
       " [0.17893028259277344],\n",
       " [0.0717601552605629],\n",
       " [0.3590797483921051],\n",
       " [1.8169963359832764],\n",
       " [1.2003259658813477],\n",
       " [0.20997576415538788],\n",
       " [0.18222232162952423],\n",
       " [0.9577623605728149],\n",
       " [1.0352948904037476],\n",
       " [0.0660235732793808],\n",
       " [0.23979230225086212],\n",
       " [1.6157182455062866],\n",
       " [0.15838831663131714],\n",
       " [0.15777719020843506],\n",
       " [0.3193759620189667],\n",
       " [0.9293957352638245],\n",
       " [0.23564961552619934],\n",
       " [0.4339229464530945],\n",
       " [1.0556445121765137],\n",
       " [0.7876957654953003],\n",
       " [1.0922185182571411],\n",
       " [0.10933175683021545],\n",
       " [1.3034693002700806],\n",
       " [0.18184560537338257],\n",
       " [0.3685341477394104],\n",
       " [0.13849188387393951],\n",
       " [0.3607438802719116],\n",
       " [1.9405393600463867],\n",
       " [0.2732595205307007],\n",
       " [1.681382179260254],\n",
       " [0.530149519443512],\n",
       " [0.2893405556678772],\n",
       " [1.4530633687973022],\n",
       " [0.09969424456357956],\n",
       " [0.8744421005249023],\n",
       " [1.1222898960113525],\n",
       " [1.0930849313735962],\n",
       " [2.014662504196167],\n",
       " [0.37318193912506104],\n",
       " [0.15488161146640778],\n",
       " [0.2542957663536072],\n",
       " [1.8607966899871826],\n",
       " [0.2802932560443878],\n",
       " [0.37854430079460144],\n",
       " [0.2660922408103943],\n",
       " [1.3181676864624023],\n",
       " [1.5014406442642212],\n",
       " [0.11462386697530746],\n",
       " [1.1555323600769043],\n",
       " [1.1557544469833374],\n",
       " [0.14963510632514954],\n",
       " [0.21875527501106262],\n",
       " [0.8772370219230652],\n",
       " [0.48029932379722595],\n",
       " [1.0069098472595215],\n",
       " [0.43178239464759827],\n",
       " [1.0371689796447754],\n",
       " [0.2862033545970917],\n",
       " [0.41711291670799255],\n",
       " [0.2734615206718445],\n",
       " [0.2278471291065216],\n",
       " [1.1075496673583984],\n",
       " [0.910200297832489],\n",
       " [0.04692818224430084],\n",
       " [0.5529764294624329],\n",
       " [0.06752149015665054],\n",
       " [0.18091252446174622],\n",
       " [0.16742195188999176],\n",
       " [0.25577259063720703],\n",
       " [1.8145383596420288],\n",
       " [0.7923168540000916],\n",
       " [1.057400107383728],\n",
       " [1.0295004844665527],\n",
       " [0.9425433874130249],\n",
       " [1.1671820878982544],\n",
       " [1.381933569908142],\n",
       " [2.7721059322357178],\n",
       " [0.07122431695461273],\n",
       " [0.9221493005752563],\n",
       " [0.34545594453811646],\n",
       " [0.46006181836128235],\n",
       " [0.17986609041690826],\n",
       " [3.175637722015381],\n",
       " [0.058400217443704605],\n",
       " [0.05225449427962303],\n",
       " [0.44001492857933044],\n",
       " [0.8373226523399353],\n",
       " [1.8032801151275635],\n",
       " [1.2023848295211792],\n",
       " [3.2393696308135986],\n",
       " [0.5302349328994751],\n",
       " [0.5316089987754822],\n",
       " [0.5842043161392212],\n",
       " [0.9456470012664795],\n",
       " [0.7020214200019836],\n",
       " [0.9829650521278381],\n",
       " [0.20894187688827515],\n",
       " [0.29444047808647156],\n",
       " [1.304587960243225],\n",
       " [0.4118462800979614],\n",
       " [1.155536413192749],\n",
       " [0.2559097707271576],\n",
       " [0.9073103070259094],\n",
       " [0.5676772594451904],\n",
       " [0.20862868428230286],\n",
       " [0.9785470962524414],\n",
       " [0.20234329998493195],\n",
       " [0.15157757699489594],\n",
       " [0.2942955791950226],\n",
       " [0.3031602203845978],\n",
       " [0.6911054253578186],\n",
       " [1.0659093856811523],\n",
       " [0.3070269227027893],\n",
       " [1.706308126449585],\n",
       " [0.7857478260993958],\n",
       " [0.35990121960639954],\n",
       " [0.21159115433692932],\n",
       " [0.25902682542800903],\n",
       " [0.346602201461792],\n",
       " [1.6602517366409302],\n",
       " [1.075121521949768],\n",
       " [1.8189537525177002],\n",
       " [0.7213691473007202],\n",
       " [0.14118312299251556],\n",
       " [0.22737354040145874],\n",
       " [0.7478030323982239],\n",
       " [0.19899919629096985],\n",
       " [1.600887417793274],\n",
       " [0.31027287244796753],\n",
       " [0.24316345155239105],\n",
       " [1.5962358713150024],\n",
       " [0.6457472443580627],\n",
       " [0.200449138879776],\n",
       " [0.09775040298700333],\n",
       " [0.9160563945770264],\n",
       " [1.1666563749313354],\n",
       " [0.12523508071899414],\n",
       " [0.5096840858459473],\n",
       " [0.3595033586025238],\n",
       " [0.25040847063064575],\n",
       " [1.3820984363555908],\n",
       " [1.022847056388855],\n",
       " [0.2390761822462082],\n",
       " [1.6579476594924927],\n",
       " [0.10568192601203918],\n",
       " [1.6494030952453613],\n",
       " [0.11478042602539062],\n",
       " [0.40325140953063965],\n",
       " [0.9112550020217896],\n",
       " [0.46948152780532837],\n",
       " [0.24972271919250488],\n",
       " [0.10770750790834427],\n",
       " [0.4862053096294403],\n",
       " [0.30265843868255615],\n",
       " [0.4781331419944763],\n",
       " [1.209499716758728],\n",
       " [0.8013207912445068],\n",
       " [0.5099180936813354],\n",
       " [1.022748589515686],\n",
       " [0.4563201069831848],\n",
       " [1.0766133069992065],\n",
       " [0.17852896451950073],\n",
       " [0.10510677099227905],\n",
       " [0.2983132004737854],\n",
       " [0.2120971530675888],\n",
       " [0.12714584171772003],\n",
       " [1.4680622816085815],\n",
       " [0.14653456211090088],\n",
       " [1.0718164443969727],\n",
       " [0.5471548438072205],\n",
       " [0.8641843795776367],\n",
       " [0.05350128933787346],\n",
       " [0.7839867472648621],\n",
       " [0.7661264538764954],\n",
       " [0.5796241760253906],\n",
       " [0.7239830493927002],\n",
       " [0.6331077218055725],\n",
       " [1.765453577041626],\n",
       " [0.43648040294647217],\n",
       " [0.49518921971321106],\n",
       " [0.604295015335083],\n",
       " [0.7763477563858032],\n",
       " [1.9080250263214111],\n",
       " [0.8073025941848755],\n",
       " [0.6040073037147522],\n",
       " [0.49480387568473816],\n",
       " [0.6394309401512146],\n",
       " [0.7482783794403076],\n",
       " [1.0710017681121826],\n",
       " [0.3576441705226898],\n",
       " [0.49350211024284363],\n",
       " [0.8229094743728638],\n",
       " [0.5933217406272888],\n",
       " [1.158083200454712],\n",
       " [1.0303375720977783],\n",
       " [1.0883820056915283],\n",
       " [1.340919017791748],\n",
       " [0.7404871582984924],\n",
       " [0.46867960691452026],\n",
       " [0.9850542545318604],\n",
       " [0.9959568977355957],\n",
       " [0.9918220043182373],\n",
       " [0.41922345757484436],\n",
       " [0.4931221306324005],\n",
       " [1.046086311340332],\n",
       " [1.784864902496338],\n",
       " [0.520900309085846],\n",
       " [0.41152018308639526],\n",
       " [0.5796013474464417],\n",
       " [0.7761803865432739],\n",
       " [0.7568031549453735],\n",
       " [1.1212271451950073],\n",
       " [0.6333314180374146],\n",
       " [0.7200992107391357],\n",
       " [0.47006478905677795],\n",
       " [0.501693844795227],\n",
       " [1.030689001083374],\n",
       " [0.7620747685432434],\n",
       " [1.2461109161376953],\n",
       " [0.5918148159980774],\n",
       " [0.35912537574768066],\n",
       " [0.9450512528419495],\n",
       " [0.7518079876899719],\n",
       " [0.9225432276725769],\n",
       " [0.5685072541236877],\n",
       " [0.9718736410140991],\n",
       " [0.7368695735931396],\n",
       " [1.163812518119812],\n",
       " [0.6521665453910828],\n",
       " [1.3453036546707153],\n",
       " [0.6001672148704529],\n",
       " [0.8945257663726807],\n",
       " [0.1649693101644516],\n",
       " [0.7782127261161804],\n",
       " [1.8944225311279297],\n",
       " [0.9529026746749878],\n",
       " [0.5423178672790527],\n",
       " [0.7044762372970581],\n",
       " [1.0050517320632935],\n",
       " [0.6632592678070068],\n",
       " [1.4697902202606201],\n",
       " [0.7761843800544739],\n",
       " [0.4090260863304138],\n",
       " [0.8527629971504211],\n",
       " [0.5054636001586914],\n",
       " [0.6786102056503296],\n",
       " [0.36209753155708313],\n",
       " [1.6877801418304443],\n",
       " [0.3566265404224396],\n",
       " [0.3354727327823639],\n",
       " [1.1603273153305054],\n",
       " [0.6208968162536621],\n",
       " [1.4129363298416138],\n",
       " [1.3719266653060913],\n",
       " [0.6435457468032837],\n",
       " [0.8614554405212402],\n",
       " [0.6720362901687622],\n",
       " [0.6599990725517273],\n",
       " [0.6724560260772705],\n",
       " [1.320648193359375],\n",
       " [1.3043978214263916],\n",
       " [0.9408707022666931],\n",
       " [0.15116359293460846],\n",
       " [1.3747228384017944],\n",
       " [0.637669026851654],\n",
       " [0.8959547877311707],\n",
       " [0.3655519485473633],\n",
       " [0.8074530363082886],\n",
       " [0.4868072271347046],\n",
       " [0.7343659996986389],\n",
       " [0.649666428565979],\n",
       " [0.3523000180721283],\n",
       " [0.5813981890678406],\n",
       " [1.818017601966858],\n",
       " [0.6640638113021851],\n",
       " [0.7952219843864441],\n",
       " [0.5046758055686951],\n",
       " [2.196960926055908],\n",
       " [0.6494321227073669],\n",
       " [1.7441073656082153],\n",
       " [1.2619926929473877],\n",
       " [0.7677826881408691],\n",
       " [0.4730488955974579],\n",
       " [0.5685646533966064],\n",
       " [0.5023581385612488],\n",
       " [0.38255253434181213],\n",
       " [1.792060136795044],\n",
       " [1.2771166563034058],\n",
       " [0.39452385902404785],\n",
       " [0.21477875113487244],\n",
       " [0.7622936964035034],\n",
       " [0.4514792859554291],\n",
       " [0.8056856393814087],\n",
       " [0.31705188751220703],\n",
       " [0.3033650815486908],\n",
       " [0.4213500916957855],\n",
       " [1.8161038160324097],\n",
       " [0.9393095970153809],\n",
       " [0.8735530972480774],\n",
       " [0.5767605900764465],\n",
       " [0.572694718837738],\n",
       " [0.6125907897949219],\n",
       " [0.6976380348205566],\n",
       " [0.6012715697288513],\n",
       " [0.8731294274330139],\n",
       " [1.483863353729248],\n",
       " [0.500352680683136],\n",
       " [0.7636083960533142],\n",
       " [0.7042771577835083],\n",
       " [1.3541659116744995],\n",
       " [0.9903900027275085],\n",
       " [0.7217047214508057],\n",
       " [1.4824671745300293],\n",
       " [0.6680580377578735],\n",
       " [0.593889057636261],\n",
       " [0.7514746189117432],\n",
       " [0.6971566081047058],\n",
       " [0.48207399249076843],\n",
       " [0.29095298051834106],\n",
       " [0.8468858003616333],\n",
       " [0.5619553923606873],\n",
       " [0.42240169644355774],\n",
       " [0.6484352350234985],\n",
       " [0.8246057033538818],\n",
       " [0.7753063440322876],\n",
       " [2.463956832885742],\n",
       " [0.8066827654838562],\n",
       " [0.5927515625953674],\n",
       " [0.8558563590049744],\n",
       " [0.23903603851795197],\n",
       " [0.6912882924079895],\n",
       " [0.8477481603622437],\n",
       " [0.90290367603302],\n",
       " [0.5742219686508179],\n",
       " [0.6730400323867798],\n",
       " [0.4581490457057953],\n",
       " [0.866594135761261],\n",
       " [0.6332257390022278],\n",
       " [0.5951412320137024],\n",
       " [0.4889054000377655],\n",
       " [0.7706970572471619],\n",
       " [0.5759447813034058],\n",
       " [0.48750102519989014],\n",
       " [0.5714539289474487],\n",
       " [1.103000283241272],\n",
       " [0.6741045713424683],\n",
       " [0.9297266006469727],\n",
       " [0.5575872659683228],\n",
       " [1.3250242471694946],\n",
       " [0.9818154573440552],\n",
       " [0.87522292137146],\n",
       " [1.288918375968933],\n",
       " [1.5314584970474243],\n",
       " [0.4000636041164398],\n",
       " [0.8152397871017456],\n",
       " [0.4633384048938751],\n",
       " [0.852990984916687],\n",
       " [0.5636268258094788],\n",
       " [0.8693588376045227],\n",
       " [0.5299845933914185],\n",
       " [0.6462845206260681],\n",
       " [0.4126061499118805],\n",
       " [0.09534525126218796],\n",
       " [0.8441301584243774],\n",
       " [0.45339062809944153],\n",
       " [0.7598049640655518],\n",
       " [1.0054733753204346],\n",
       " [0.3491646349430084],\n",
       " [1.0384478569030762],\n",
       " [2.1860005855560303],\n",
       " [2.17724871635437],\n",
       " [0.5791904926300049],\n",
       " [1.249595046043396],\n",
       " [0.4192807078361511],\n",
       " [1.5929255485534668],\n",
       " [1.1574136018753052],\n",
       " [0.7363942861557007],\n",
       " [0.611435055732727],\n",
       " [1.0826469659805298],\n",
       " [0.723395049571991],\n",
       " [1.374866008758545],\n",
       " [0.5038502216339111],\n",
       " [0.9107070565223694],\n",
       " [0.3731631338596344],\n",
       " [0.643904447555542],\n",
       " [0.573238730430603],\n",
       " [0.47667574882507324],\n",
       " [0.522108256816864],\n",
       " [0.6185783743858337],\n",
       " [0.7207253575325012],\n",
       " [0.2776816487312317],\n",
       " [0.46294164657592773],\n",
       " [0.4393305480480194],\n",
       " [0.9225320816040039],\n",
       " [1.2208791971206665],\n",
       " [0.3778960108757019],\n",
       " [0.796735942363739],\n",
       " [1.0143474340438843],\n",
       " [0.7819523811340332],\n",
       " [0.7862329483032227],\n",
       " [0.48365625739097595],\n",
       " [0.7430828809738159],\n",
       " [0.4635072350502014],\n",
       " [1.3244374990463257],\n",
       " [2.0528652667999268],\n",
       " [0.16523732244968414],\n",
       " [2.404742479324341],\n",
       " [0.3763648569583893],\n",
       " [1.6190319061279297],\n",
       " [0.5429590344429016],\n",
       " [0.6489671468734741],\n",
       " [0.22428417205810547],\n",
       " [0.9116576313972473],\n",
       " [0.9952176809310913],\n",
       " [1.3713661432266235],\n",
       " [1.2722060680389404],\n",
       " [0.5627864003181458],\n",
       " [0.6536626219749451],\n",
       " [0.3079397678375244],\n",
       " [1.3769562244415283],\n",
       " [1.015866756439209],\n",
       " [0.8825551271438599],\n",
       " [0.923352837562561],\n",
       " [0.5129170417785645],\n",
       " [0.5083032846450806],\n",
       " [0.43283629417419434],\n",
       " [0.34418943524360657],\n",
       " [0.4395540654659271],\n",
       " [1.1609368324279785],\n",
       " [0.3041553199291229],\n",
       " [1.7727762460708618],\n",
       " [0.869327187538147],\n",
       " [0.7849425077438354],\n",
       " [0.597379744052887],\n",
       " [0.9232672452926636],\n",
       " [1.2068943977355957],\n",
       " [0.9993870258331299],\n",
       " [0.6475929617881775],\n",
       " [0.9299840331077576],\n",
       " [0.7711803317070007],\n",
       " [1.2917470932006836],\n",
       " [0.9724584221839905],\n",
       " [0.5472118258476257],\n",
       " [0.44968095421791077],\n",
       " [1.0595654249191284],\n",
       " [2.0072567462921143],\n",
       " [0.5511993169784546],\n",
       " [1.5265192985534668],\n",
       " [1.6878585815429688],\n",
       " [0.6105814576148987],\n",
       " [0.47568875551223755],\n",
       " [0.6089274287223816],\n",
       " [0.8823601007461548],\n",
       " [0.5837128162384033],\n",
       " [0.6233370900154114],\n",
       " [0.5688958764076233],\n",
       " [0.4778885841369629],\n",
       " [0.6066166758537292],\n",
       " [0.9001055955886841],\n",
       " [0.6695306301116943],\n",
       " [1.2277950048446655],\n",
       " [0.9217987060546875],\n",
       " [0.6939681768417358],\n",
       " [0.4235609769821167],\n",
       " [0.37114062905311584],\n",
       " [1.2244510650634766],\n",
       " [0.9229307174682617],\n",
       " [0.6889916658401489],\n",
       " [0.8629929423332214],\n",
       " [0.7323741912841797],\n",
       " [0.31503990292549133],\n",
       " [1.0616719722747803],\n",
       " [0.15890301764011383],\n",
       " [0.6509377956390381],\n",
       " [0.6694243550300598],\n",
       " [1.1709028482437134],\n",
       " [1.061531662940979],\n",
       " [1.0365467071533203],\n",
       " [0.8112534880638123],\n",
       " [1.395707368850708],\n",
       " [1.0562721490859985],\n",
       " [0.4484235346317291],\n",
       " [0.12301856279373169],\n",
       " [0.8128491640090942],\n",
       " [1.1854175329208374],\n",
       " [0.42768049240112305],\n",
       " [0.7463812828063965],\n",
       " [0.4123498499393463],\n",
       " [0.7156437039375305],\n",
       " [0.18637248873710632],\n",
       " [0.4624650776386261],\n",
       " [0.49267831444740295],\n",
       " [0.5618885159492493],\n",
       " [0.9342275857925415],\n",
       " [0.6406154036521912],\n",
       " [0.8267095685005188],\n",
       " [1.1551769971847534],\n",
       " [0.6233918070793152],\n",
       " [0.5044885277748108],\n",
       " [0.15316154062747955],\n",
       " [1.3227663040161133],\n",
       " [0.5875169038772583],\n",
       " [0.7531870007514954],\n",
       " [0.5108721256256104],\n",
       " [0.6253048777580261],\n",
       " [1.2253247499465942],\n",
       " [0.3906756639480591],\n",
       " [1.3162245750427246],\n",
       " [0.8009548783302307],\n",
       " [0.6077515482902527],\n",
       " [0.18808037042617798],\n",
       " [0.4582815170288086],\n",
       " [0.5102901458740234],\n",
       " [1.0480504035949707],\n",
       " [0.34380215406417847],\n",
       " [0.632988691329956],\n",
       " [0.9595033526420593],\n",
       " [1.1223971843719482],\n",
       " [1.4958250522613525],\n",
       " [0.679929792881012],\n",
       " [0.5659884810447693],\n",
       " [1.1586045026779175],\n",
       " [0.0992882177233696],\n",
       " [2.8238677978515625],\n",
       " [0.294107049703598],\n",
       " [0.7585416436195374],\n",
       " [0.5807589292526245],\n",
       " [1.6329535245895386],\n",
       " [0.527579665184021],\n",
       " [0.5515455007553101],\n",
       " [0.9533094763755798],\n",
       " [1.2169158458709717],\n",
       " [0.5511783957481384],\n",
       " [0.748735785484314],\n",
       " [0.9238322973251343],\n",
       " [0.97941654920578],\n",
       " [1.0720540285110474],\n",
       " [1.0769370794296265],\n",
       " [0.6894478797912598],\n",
       " [1.0445865392684937],\n",
       " [1.548407793045044],\n",
       " [0.3417956233024597],\n",
       " [0.8438023328781128],\n",
       " [1.0400772094726562],\n",
       " [2.063185453414917],\n",
       " [0.26395562291145325],\n",
       " [0.6182448863983154],\n",
       " [0.44094932079315186],\n",
       " [1.4496713876724243],\n",
       " [1.0938720703125],\n",
       " [0.5920807719230652],\n",
       " [0.5443479418754578],\n",
       " [0.22059106826782227],\n",
       " [0.5327337980270386],\n",
       " [0.43080171942710876],\n",
       " [0.5506879091262817],\n",
       " [0.4745468199253082],\n",
       " [0.7138035893440247],\n",
       " [1.1800034046173096],\n",
       " [0.6187201738357544],\n",
       " [0.47189265489578247],\n",
       " [0.7360033392906189],\n",
       " [0.5337295532226562],\n",
       " [0.7827644944190979],\n",
       " [0.6474214196205139],\n",
       " [1.3056373596191406],\n",
       " [0.8176134824752808],\n",
       " [1.2156157493591309],\n",
       " [0.45228347182273865],\n",
       " [0.7308028340339661],\n",
       " [1.0227423906326294],\n",
       " [0.6148195266723633],\n",
       " [1.7848621606826782],\n",
       " [0.8589041233062744],\n",
       " [0.8873052597045898],\n",
       " [1.0907208919525146],\n",
       " [0.3788323998451233],\n",
       " [0.4487093687057495],\n",
       " [1.2955186367034912],\n",
       " [0.4577239155769348],\n",
       " [0.7860127687454224],\n",
       " [1.1373116970062256],\n",
       " [0.6682351231575012],\n",
       " [2.205111265182495],\n",
       " [0.23529018461704254],\n",
       " [0.6806652545928955],\n",
       " [1.054863452911377],\n",
       " [0.5846689939498901],\n",
       " [0.7574028968811035],\n",
       " [0.4305211305618286],\n",
       " [1.1746361255645752],\n",
       " [0.893174946308136],\n",
       " [1.6931536197662354],\n",
       " [0.5738804936408997],\n",
       " [0.7561426758766174],\n",
       " [1.147796869277954],\n",
       " [0.8889392614364624],\n",
       " [1.3709981441497803],\n",
       " [0.5348029136657715],\n",
       " [0.7514126300811768],\n",
       " [0.3098732531070709],\n",
       " [0.39828166365623474],\n",
       " [1.0436409711837769],\n",
       " [0.7148705124855042],\n",
       " [0.8179475665092468],\n",
       " [0.8691214323043823],\n",
       " [1.5857441425323486],\n",
       " [1.663835883140564],\n",
       " [1.3276236057281494],\n",
       " [1.2897166013717651],\n",
       " [0.7257120013237],\n",
       " [1.2424455881118774],\n",
       " [0.5307460427284241],\n",
       " [1.1018999814987183],\n",
       " [0.5709162354469299],\n",
       " [1.1886080503463745],\n",
       " [1.154887318611145],\n",
       " [1.0216262340545654],\n",
       " [0.6568540930747986],\n",
       " [0.529312789440155],\n",
       " [0.6561129093170166],\n",
       " [0.4637238681316376],\n",
       " [0.5608746409416199],\n",
       " [0.6234796047210693],\n",
       " [0.9003841876983643],\n",
       " [0.7453128099441528],\n",
       " [0.33669325709342957],\n",
       " [0.7104066014289856],\n",
       " [0.5821211338043213],\n",
       " [1.162205696105957],\n",
       " [0.9806156754493713],\n",
       " [1.458173394203186],\n",
       " [0.3994539678096771],\n",
       " [0.9609208106994629],\n",
       " [0.5976763367652893],\n",
       " [0.24527369439601898],\n",
       " [1.0597025156021118],\n",
       " [1.0816419124603271],\n",
       " [1.4643384218215942],\n",
       " [0.8978912830352783],\n",
       " [0.7922078371047974],\n",
       " [1.1234018802642822],\n",
       " [0.6231757998466492],\n",
       " [0.8903582096099854],\n",
       " [1.3525452613830566],\n",
       " [0.580436646938324],\n",
       " [0.7218468189239502],\n",
       " [0.9171161651611328],\n",
       " [1.094842553138733],\n",
       " [0.8037083745002747],\n",
       " [0.9634576439857483],\n",
       " [0.9293352365493774],\n",
       " [1.1193336248397827],\n",
       " [0.8974575996398926],\n",
       " [0.9805017709732056],\n",
       " [0.7028216123580933],\n",
       " [0.6434215307235718],\n",
       " [0.7101203203201294],\n",
       " [0.8686904311180115],\n",
       " [0.7952280640602112],\n",
       " [0.95918208360672],\n",
       " [1.0044279098510742],\n",
       " [0.5470626950263977],\n",
       " [0.5055103302001953],\n",
       " [0.9249032735824585],\n",
       " [0.5068550109863281],\n",
       " [0.8173540830612183],\n",
       " [0.8828294277191162],\n",
       " [0.7805686593055725],\n",
       " [1.0333291292190552],\n",
       " [1.5681148767471313],\n",
       " [1.0498687028884888],\n",
       " [0.8773394823074341],\n",
       " [0.9918211102485657],\n",
       " [1.1072558164596558],\n",
       " [0.7630402445793152],\n",
       " [1.0313224792480469],\n",
       " [1.0952552556991577],\n",
       " [0.6477834582328796],\n",
       " [1.7084828615188599],\n",
       " [0.7503793239593506],\n",
       " [0.9456984400749207],\n",
       " [0.49682870507240295],\n",
       " [0.299979567527771],\n",
       " [2.6673641204833984],\n",
       " [1.4743162393569946],\n",
       " [0.7555313110351562],\n",
       " [0.8201332092285156],\n",
       " [0.5940312147140503],\n",
       " [0.4926023483276367],\n",
       " [0.8256969451904297],\n",
       " [0.6499340534210205],\n",
       " [1.7369176149368286],\n",
       " [0.4990084767341614],\n",
       " [0.5753562450408936],\n",
       " [0.6342332363128662],\n",
       " [1.2951414585113525],\n",
       " [0.6780881881713867],\n",
       " [1.8070042133331299],\n",
       " [0.6783812046051025],\n",
       " [0.6602272391319275],\n",
       " [0.3820832669734955],\n",
       " [1.2093673944473267],\n",
       " [1.075221061706543],\n",
       " [0.37013375759124756],\n",
       " [0.8382708430290222],\n",
       " [1.467397689819336],\n",
       " [0.3200185298919678],\n",
       " [0.4922201931476593],\n",
       " [0.8187112212181091],\n",
       " [1.2745251655578613],\n",
       " [0.6147809624671936],\n",
       " [1.2039414644241333],\n",
       " [0.26146650314331055],\n",
       " [0.9409094452857971],\n",
       " [1.3038827180862427],\n",
       " [1.9401054382324219],\n",
       " [2.0104849338531494],\n",
       " [0.8551259636878967],\n",
       " [1.00343918800354],\n",
       " [0.8869622349739075],\n",
       " [0.4989146292209625],\n",
       " [1.3153687715530396],\n",
       " [0.582542896270752],\n",
       " [0.6336119771003723],\n",
       " [0.7174967527389526],\n",
       " [1.2421766519546509],\n",
       " [1.2974929809570312],\n",
       " [0.6497023701667786],\n",
       " [0.6831153035163879],\n",
       " [1.2269771099090576],\n",
       " [0.5707603693008423],\n",
       " [0.6713497638702393],\n",
       " [0.6612685322761536],\n",
       " [0.6035866737365723],\n",
       " [1.0108643770217896],\n",
       " [0.3026938736438751],\n",
       " [1.8871413469314575],\n",
       " [0.5820514559745789],\n",
       " [0.46454131603240967],\n",
       " [0.4474002718925476],\n",
       " [0.5712676644325256],\n",
       " [0.9096776247024536],\n",
       " [1.639264464378357],\n",
       " [1.1457383632659912],\n",
       " [0.8467350602149963],\n",
       " [0.6216522455215454],\n",
       " [0.9337857365608215],\n",
       " [0.6409043669700623],\n",
       " [1.3508045673370361],\n",
       " [0.6517293453216553],\n",
       " [1.3526707887649536],\n",
       " [0.8009778261184692],\n",
       " [0.6312385201454163],\n",
       " [0.9450421929359436],\n",
       " [0.9366480112075806],\n",
       " [0.6071262955665588],\n",
       " [0.5163148045539856],\n",
       " [1.1487987041473389],\n",
       " [0.42163020372390747],\n",
       " [0.7565865516662598],\n",
       " [0.5401576161384583],\n",
       " [0.8520123958587646],\n",
       " [0.49529027938842773],\n",
       " [0.10960137099027634],\n",
       " [0.7589861750602722],\n",
       " [0.5329453349113464],\n",
       " [0.9049222469329834],\n",
       " [0.8957012295722961],\n",
       " [0.4549441635608673],\n",
       " [0.9028672575950623],\n",
       " [0.4957691431045532],\n",
       " [1.1646250486373901],\n",
       " [0.8188052773475647],\n",
       " [0.8987972140312195],\n",
       " [0.4728074073791504],\n",
       " [1.0137603282928467],\n",
       " [0.6365648508071899],\n",
       " [0.8638864159584045],\n",
       " [0.68719482421875],\n",
       " [0.5726411938667297],\n",
       " [0.565304160118103],\n",
       " [1.0086358785629272],\n",
       " [0.6512705087661743],\n",
       " [0.6554156541824341],\n",
       " [0.4718228578567505],\n",
       " [0.6711278557777405],\n",
       " [0.10754180699586868],\n",
       " [0.7225856781005859],\n",
       " [0.9248794913291931],\n",
       " [0.6408500671386719],\n",
       " [0.30278322100639343],\n",
       " [0.9018347859382629],\n",
       " [0.6943633556365967],\n",
       " [0.4002889394760132],\n",
       " [1.4249228239059448],\n",
       " [0.3429834544658661],\n",
       " [0.8868123888969421],\n",
       " [1.3311446905136108],\n",
       " [0.5754154920578003],\n",
       " [0.09452252089977264],\n",
       " [0.5871682167053223],\n",
       " [0.5491344332695007],\n",
       " [0.46145808696746826],\n",
       " [0.6605210900306702],\n",
       " [0.8652303218841553],\n",
       " [1.8961280584335327],\n",
       " [0.21706733107566833],\n",
       " [0.4075477123260498],\n",
       " [0.8035571575164795],\n",
       " [0.9551317691802979],\n",
       " [1.3812243938446045],\n",
       " [0.28109097480773926],\n",
       " [0.560299813747406],\n",
       " [0.9616281986236572],\n",
       " [0.5760964155197144],\n",
       " [1.4433842897415161],\n",
       " [1.803543210029602],\n",
       " [1.0608550310134888],\n",
       " [0.8270852565765381],\n",
       " [0.7683857679367065],\n",
       " [0.6458278298377991],\n",
       " [0.9038943648338318],\n",
       " [0.44735828042030334],\n",
       " [1.2169153690338135],\n",
       " [0.4976666569709778],\n",
       " [0.9049481749534607],\n",
       " [0.9461233019828796],\n",
       " [1.1479697227478027],\n",
       " [0.9110619425773621],\n",
       " [0.7925799489021301],\n",
       " [0.933882474899292],\n",
       " [0.41515976190567017],\n",
       " [1.0028132200241089],\n",
       " [1.077201247215271],\n",
       " [0.7344810962677002],\n",
       " [1.1458591222763062],\n",
       " [1.4014778137207031],\n",
       " [1.4082896709442139],\n",
       " [1.051106333732605],\n",
       " [1.1517452001571655],\n",
       " [0.6314195394515991],\n",
       " [1.1107228994369507],\n",
       " [0.47068172693252563],\n",
       " [0.848163902759552],\n",
       " [0.5822509527206421],\n",
       " [0.7849050164222717],\n",
       " [0.7791702151298523],\n",
       " [0.7895889282226562],\n",
       " [0.5740761756896973],\n",
       " [0.49012723565101624],\n",
       " [0.8473389744758606],\n",
       " [0.7954736351966858],\n",
       " [0.7131043672561646],\n",
       " [0.6831270456314087],\n",
       " [1.1930363178253174],\n",
       " [0.25433096289634705],\n",
       " [1.1424784660339355],\n",
       " [1.416578769683838],\n",
       " [1.7015405893325806],\n",
       " [1.3297556638717651],\n",
       " [0.8121767044067383],\n",
       " [1.0909146070480347],\n",
       " [0.42020827531814575],\n",
       " [1.3058931827545166],\n",
       " [0.7877873778343201],\n",
       " [0.8897520303726196],\n",
       " [0.775021493434906],\n",
       " [0.39526811242103577],\n",
       " [2.0199050903320312],\n",
       " [0.7440009713172913],\n",
       " [1.253854751586914],\n",
       " [1.114529013633728],\n",
       " [0.8448119163513184],\n",
       " [0.1863640397787094],\n",
       " [0.5751403570175171],\n",
       " [0.8010754585266113],\n",
       " [0.2480890303850174],\n",
       " [1.1427193880081177],\n",
       " [1.5034464597702026],\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
